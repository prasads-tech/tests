graph TD
    A[Start: Push to feature/dv1 branch] --> B[Checkout code]

    subgraph Setup Environment
        B --> C[Set up jq]
        C --> D[Set up Python 3.9]
        D --> E[Cache Python dependencies]
        E --> F[Install dependencies]
        F --> G[Set up Git]
    end

    G --> H[Get newly added or modified JSON files]

    subgraph Prepare Inputs
        H --> I[Extract Parent Folder]
        I --> J[Choose Environment and Load Variables]
        J --> K[Set environment-specific secrets]
        K --> L[List and Process JSON Files]
    end

    L --> M[Run main Python script]

    subgraph Git Operations
        M --> N[Generate JWT for GitHub App]
        N --> O[Get GitHub Installation Access Token]
        O --> P[Configure git]
        P --> Q[Stash untracked files]
        Q --> R[Switch to new branch]
        R --> S[Pull changes and rebase]
        S --> T[Reapply stashed changes]
        T --> U[Commit and push changes]
    end

    U --> V[Install ArgoCD CLI]

    subgraph ArgoCD Operations
        V --> W[Login to ArgoCD]
        W --> X[Create ArgoCD app for workload cluster]
        X --> Y[Sync ArgoCD app]
        Y --> Z[Install kubectl]
        Z --> AA[Configure kubectl]
        AA --> AB[Install clusterctl]
        AB --> AC[Verify workload cluster is provisioned]
        AC --> AD[Attach ACR to AKS cluster]
        AD --> AE[Add cluster to ArgoCD]
        AE --> AF[Verify ArgoCD apps are synced]
    end

    AF --> AG[End]


Descriptions of Each Step:
Push to feature/XYZ branch: The developer pushes code to the feature branch, which triggers the pipeline.
Checkout code: The GitHub Action checks out the latest code from the repository.
Set up jq: jq is installed to handle JSON file processing in the next steps.
Set up Python: Python is set up to run Python scripts in the pipeline.
Cache Python dependencies: Dependencies from previous runs are cached to reduce build time.
Install dependencies: Any remaining or new dependencies are installed.
Set up Git: Git is configured to allow later commits and pushes.
Get newly added or modified JSON files: Identify new or modified input.json files to be processed.
Extract Parent Folder: Extract the folder from the JSON files to prepare for environment-specific processing.
Choose Environment and Load Variables: Based on the environment (dev/prod), load the corresponding variables.
Set environment-specific secrets: Load environment-specific secrets and sensitive data.
List and Process JSON Files: List the JSON files that will be processed and validate them.
Run main Python script: The main Python script is executed, generating the Kubernetes manifests.
Generate JWT for GitHub App: A JWT is generated for authenticating with GitHub as a GitHub App.
Get GitHub Installation Access Token: Use the JWT to get an installation access token for interacting with GitHub.
Configure Git: Git is configured with the required credentials for committing and pushing changes.
Stash untracked files: Temporarily stash any untracked files to avoid conflicts when switching branches.
Switch to a new branch: Switch to a new feature branch where the changes will be pushed.
Pull changes and rebase: Pull changes from the remote repository and rebase the current branch to avoid conflicts.
Reapply stashed changes: Reapply the stashed changes to the new branch.
Commit and push changes: Commit and push the changes (including manifests) to the new branch.
Install ArgoCD CLI: Install the ArgoCD CLI for managing ArgoCD operations.
Login to ArgoCD: Log into ArgoCD to manage applications and clusters.
Create an ArgoCD app for the workload cluster: Create an ArgoCD application for deploying the workload cluster.
Sync ArgoCD app: Sync the ArgoCD app to ensure that the workload cluster is deployed correctly.
Install kubectl: Install the kubectl CLI to interact with the Kubernetes cluster.
Configure kubectl: Configure kubectl with credentials to manage the cluster.
Install clusterctl: Install the clusterctl CLI for managing Kubernetes clusters.
Verify workload cluster is provisioned: Verify that the workload cluster has been successfully provisioned.
Attach ACR to AKS cluster: Attach the Azure Container Registry (ACR) to the AKS cluster for pulling container images.
Add a cluster to ArgoCD: Add the new AKS cluster to ArgoCD for application management.
Verify that ArgoCD apps are synced: Ensure that all ArgoCD applications are synced with the current configuration.
End: The process ends after successful deployment and verification.


echo "${{ secrets.DEV_GITHUB_APP_PRIVATE_KEY }}" > private_key.pem
      echo "GITHUB_APP_PRIVATE_KEY_FILE=private_key.pem" >> $GITHUB_ENV

- name: Set environment-specific secrets
  run: |
    # Load environment-specific secrets based on the parent folder
    if [ "$BASE_FOLDER" = "Clusters-Dev" ]; then
      echo "ARGOPWD=${{ secrets.DEV_ARGOPWD }}" >> $GITHUB_ENV
      echo "GITHUB_APP_ID=${{ secrets.VAM_G_APP_ID_DEV }}" >> $GITHUB_ENV
      echo "INSTALLATION_ID=${{ secrets.INSTALLATION_ID_DEV }}" >> $GITHUB_ENV
      echo "GITHUB_APP_PRIVATE_KEY=${{ secrets.VAM_G_PRIVATE_KEY_DEV }}" >> $GITHUB_ENV
    elif [ "$BASE_FOLDER" = "Clusters-NonProd" ]; then
      echo "ARGOPWD=${{ secrets.NONPROD_ARGOPWD }}" >> $GITHUB_ENV
      echo "GITHUB_APP_ID=${{ secrets.VAM_G_APP_ID_NONPROD }}" >> $GITHUB_ENV
      echo "INSTALLATION_ID=${{ secrets.INSTALLATION_ID_NONPROD }}" >> $GITHUB_ENV
      echo "GITHUB_APP_PRIVATE_KEY=${{ secrets.VAM_G_PRIVATE_KEY_NONPROD }}" >> $GITHUB_ENV
    elif [ "$BASE_FOLDER" = "Clusters-Prod" ]; then
      echo "ARGOPWD=${{ secrets.PROD_ARGOPWD }}" >> $GITHUB_ENV
      echo "GITHUB_APP_ID=${{ secrets.VAM_G_APP_ID_PROD }}" >> $GITHUB_ENV
      echo "INSTALLATION_ID=${{ secrets.INSTALLATION_ID_PROD }}" >> $GITHUB_ENV
      echo "GITHUB_APP_PRIVATE_KEY=${{ secrets.VAM_G_PRIVATE_KEY_PROD }}" >> $GITHUB_ENV
    elif [ "$BASE_FOLDER" = "Clusters-sb" ]; then
      echo "ARGOPWD=${{ secrets.SB_ARGOPWD }}" >> $GITHUB_ENV
      echo "GITHUB_APP_ID=${{ secrets.VAM_G_APP_ID_SB }}" >> $GITHUB_ENV
      echo "INSTALLATION_ID=${{ secrets.INSTALLATION_ID_SB }}" >> $GITHUB_ENV
      echo "GITHUB_APP_PRIVATE_KEY=${{ secrets.VAM_G_PRIVATE_KEY_SB }}" >> $GITHUB_ENV
    else
      echo "Invalid environment"
      exit 1
    fi
  shell: bash




- name: Load Secrets for Specific Environment
        run: |
          # Based on the parent folder, load the corresponding environment-specific secrets
          if [ "$BASE_FOLDER" = "Clusters-Dev" ]; then
            echo "ARGOPWD=${{ secrets.DEV_ARGOPWD }}" >> $GITHUB_ENV
            echo "KUBECONF=${{ secrets.DEV_KUBECONF }}" >> $GITHUB_ENV
            echo "GITHUB_APP_ID=${{ secrets.VAM_G_APP_ID }}" >> $GITHUB_ENV
            echo "INSTALLATION_ID=${{ secrets.INSTALLATION_ID }}" >> $GITHUB_ENV
            echo "GITHUB_APP_PRIVATE_KEY=${{ secrets.VAM_G_PRIVATE_KEY }}" >> $GITHUB_ENV
          elif [ "$BASE_FOLDER" = "Clusters-NonProd" ]; then
            echo "ARGOPWD=${{ secrets.NONPROD_ARGOPWD }}" >> $GITHUB_ENV
            echo "KUBECONF=${{ secrets.NONPROD_KUBECONF }}" >> $GITHUB_ENV
            echo "GITHUB_APP_ID=${{ secrets.VAM_NONPROD_G_APP_ID }}" >> $GITHUB_ENV
            echo "INSTALLATION_ID=${{ secrets.NONPROD_INSTALLATION_ID }}" >> $GITHUB_ENV
            echo "GITHUB_APP_PRIVATE_KEY=${{ secrets.VAM_NONPROD_G_PRIVATE_KEY }}" >> $GITHUB_ENV
          elif [ "$BASE_FOLDER" = "Clusters-Prod" ]; then
            echo "ARGOPWD=${{ secrets.PROD_ARGOPWD }}" >> $GITHUB_ENV
            echo "KUBECONF=${{ secrets.PROD_KUBECONF }}" >> $GITHUB_ENV
            echo "GITHUB_APP_ID=${{ secrets.VAM_PROD_G_APP_ID }}" >> $GITHUB_ENV
            echo "INSTALLATION_ID=${{ secrets.PROD_INSTALLATION_ID }}" >> $GITHUB_ENV
            echo "GITHUB_APP_PRIVATE_KEY=${{ secrets.VAM_PROD_G_PRIVATE_KEY }}" >> $GITHUB_ENV
          elif [ "$BASE_FOLDER" = "Clusters-sb" ]; then
            echo "ARGOPWD=${{ secrets.SB_ARGOPWD }}" >> $GITHUB_ENV
            echo "KUBECONF=${{ secrets.SB_KUBECONF }}" >> $GITHUB_ENV
            echo "GITHUB_APP_ID=${{ secrets.VAM_SB_G_APP_ID }}" >> $GITHUB_ENV
            echo "INSTALLATION_ID=${{ secrets.SB_INSTALLATION_ID }}" >> $GITHUB_ENV
            echo "GITHUB_APP_PRIVATE_KEY=${{ secrets.VAM_SB_G_PRIVATE_KEY }}" >> $GITHUB_ENV
          else
            echo "Invalid environment"
            exit 1
          fi


echo "OUTPUT_DIR=$OUTPUT_DIR" >> $GITHUB_ENV
echo "MAPPING_PATH=$MAPPING_PATH" >> $GITHUB_ENV
echo "TEMPLATE_PATH=$TEMPLATE_PATH" >> $GITHUB_ENV

run: |
  echo "Running Python script with the following arguments:"
  echo "Input: ${NEW_FILES}"
  echo "Output: $OUTPUT_DIR"
  echo "Mapping: $MAPPING_PATH"
  echo "Template: $TEMPLATE_PATH"
  
  python Automation/AKS/Workload-Cluster/main.py \
    --input "${NEW_FILES}" \
    --output "$OUTPUT_DIR" \
    --mapping "$MAPPING_PATH" \
    --template "$TEMPLATE_PATH"


echo "OUTPUT_DIR: $OUTPUT_DIR"
echo "MAPPING_PATH: $MAPPING_PATH"
echo "TEMPLATE_PATH: $TEMPLATE_PATH"


name: mainv2

on:
  push:
    branches:
      - newaddons
    paths:
      - "Clusters-Dev/AKS/Workload/**/*.yaml"   # Triggers on added or modified YAML files

env:
  MANAGEMENT_CLUSTER_KUBECONF: ${{ secrets.KUBECONFIG }}

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Git
        run: git fetch --prune --unshallow

      - name: Get newly added or modified YAML files
        run: |
          # Fetch the latest commit and the previous commit
          new_commit=$(git rev-parse HEAD)
          prev_commit=$(git rev-parse HEAD^)

          # List YAML files that were added or modified (not deleted)
          new_files=$(git diff --name-only --diff-filter=AM $prev_commit $new_commit -- 'Clusters-Dev/AKS/Workload/**/*.yaml')
          echo "NEW_FILES=$new_files" >> $GITHUB_ENV

          # Extract the filename without its path and extension to be used as branch name, with a hardcoded prefix
          branch_name=$(basename "$new_files" | sed 's/\(.*\)\..*/\1/')
          branch_name="workload-$branch_name"
          echo "BRANCH_NAME=$branch_name" >> $GITHUB_ENV

          if [ -z "$new_files" ]; then
            echo "No YAML files found"
            exit 1
          fi

          # Get the user information (committer details)
          committer_name=$(git log -1 --pretty=format:'%an')
          committer_email=$(git log -1 --pretty=format:'%ae')
          echo "COMMITTER_NAME=$committer_name" >> $GITHUB_ENV
          echo "COMMITTER_EMAIL=$committer_email" >> $GITHUB_ENV

          # Print debug information
          echo "************************************************"
          echo "Committer Name: $committer_name"
          echo "Committer Email: $committer_email"
          echo "Branch Name: $branch_name"
          echo "Modified YAML Files: $new_files"
          echo "************************************************"






- name: Extract Parent Folder
  run: |
    # Extract the grandparent folder of the input JSON file
    parent_folder=$(basename $(dirname $(dirname $(dirname "$NEW_FILES"))))
    echo "Parent folder: $parent_folder"
    echo "BASE_FOLDER=$parent_folder" >> $GITHUB_ENV

- name: Choose Environment and Load Variables
  run: |
    # Load the correct environment file based on the parent folder
    if [ "$BASE_FOLDER" = "Clusters-Dev" ]; then
      ENV_FILE="./workflow-config/dev.env"
    elif [ "$BASE_FOLDER" = "Clusters-NonProd" ]; then
      ENV_FILE="./workflow-config/nonprod.env"
    elif [ "$BASE_FOLDER" = "Clusters-Prod" ]; then
      ENV_FILE="./workflow-config/prod.env"
    else
      echo "Invalid parent folder: $BASE_FOLDER"
      exit 1
    fi

    # Export the variables from the chosen environment file
    echo "Loading environment variables from $ENV_FILE"
    export $(grep -v '^#' "$ENV_FILE" | xargs)


- name: Extract Parent Folder
  run: |
    # Extract the parent folder of the input JSON file
    parent_folder=$(basename $(dirname $(dirname "$NEW_FILES")))
    echo "Parent folder: $parent_folder"
    echo "BASE_FOLDER=$parent_folder" >> $GITHUB_ENV

- name: Choose Environment and Load Variables
  run: |
    # Load the correct environment file based on the parent folder
    if [ "$BASE_FOLDER" = "Clusters-Dev" ]; then
      ENV_FILE="./workflow-config/dev.env"
    elif [ "$BASE_FOLDER" = "Clusters-NonProd" ]; then
      ENV_FILE="./workflow-config/nonprod.env"
    elif [ "$BASE_FOLDER" = "Clusters-Prod" ]; then
      ENV_FILE="./workflow-config/prod.env"
    else
      echo "Invalid parent folder: $BASE_FOLDER"
      exit 1
    fi

    # Export the variables from the chosen environment file
    echo "Loading environment variables from $ENV_FILE"
    export $(grep -v '^#' "$ENV_FILE" | xargs)


import json
import os
import sys
import logging
import re

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)


def load_file(filepath):
    """Load and parse a JSON file."""
    try:
        with open(filepath, 'r', encoding='utf-8') as file_handle:
            return json.load(file_handle)
    except FileNotFoundError:
        logging.error("File %s not found.", filepath)
        raise
    except json.JSONDecodeError:
        logging.error("Error decoding JSON in file %s", filepath)
        raise


def get_spec_file(input_data):
    """Get the spec file path based on the input data spec field."""
    spec_type = input_data.get("spec", "shared")  # Default to "shared" if not provided
    spec_file = f"../../../Cluster-artifacts/Spec/{spec_type}-latest.json"

    if not os.path.exists(spec_file):
        raise FileNotFoundError(f"Spec file '{spec_file}' not found for spec type: {spec_type}")
    return spec_file


def validate_input(input_data, field_mapping):
    """Validate input fields based on the rules provided in field mapping."""
    common_fields = field_mapping.get('common', {})

    for field, rules in common_fields.items():
        value = input_data.get(field)

        if not value:
            if rules.get('required', False):
                raise ValueError(
                    f"Field '{field}' is required but missing in input.json."
                )
            continue

        if not isinstance(value, str):
            raise ValueError(
                f"Field '{field}' should be a string. Got {type(value)} instead."
            )

        min_length = rules.get("min_length")
        if min_length and len(value) < min_length:
            raise ValueError(
                f"Field '{field}' is shorter than min. length of {min_length}."
            )

        max_length = rules.get("max_length")
        if max_length and len(value) > max_length:
            raise ValueError(
                f"Field '{field}' exceeds the maximum length of {max_length}."
            )

        pattern = rules.get("regex")
        if pattern and not re.match(pattern, value):
            raise ValueError(
                f"Field '{field}' does not match required pattern '{pattern}'."
            )

    logging.info("Validation successful!")


def replace_placeholders(template_data, input_data):
    """Replace placeholders in the template with values from the input data."""
    for key, value in input_data.items():
        placeholder = f"${{{key}}}"
        template_data = template_data.replace(placeholder, value)

    return template_data


def validate_paths(*paths):
    """Validate that all provided paths exist."""
    for path in paths:
        if not os.path.exists(path):
            raise FileNotFoundError(f"Path '{path}' not found.")


def create_required_folders(base_dir, folders):
    """Create required folders inside a base directory."""
    for folder in folders:
        destination_folder = os.path.join(base_dir, folder)
        os.makedirs(destination_folder, exist_ok=True)
        sample_file_path = os.path.join(destination_folder, "README.md")
        with open(sample_file_path, 'w', encoding='utf-8') as sample_file:
            sample_file.write(f"This is a sample file for {folder}.")


def generate_output(validated_data, output_dir):
    """Dynamically generate output filename and folder structure."""
    cluster_name = validated_data.get("CLUSTER_NAME")
    if not cluster_name:
        raise ValueError("CLUSTER_NAME is missing from the input data.")

    cluster_output_dir = os.path.join(output_dir, cluster_name)
    os.makedirs(cluster_output_dir, exist_ok=True)

    create_required_folders(cluster_output_dir, ["namespaces", "config", "rbac", "network"])

    output_file_name = f"{cluster_name}.yaml"
    output_file_path = os.path.join(cluster_output_dir, output_file_name)
    logging.info("Output file path: %s", output_file_path)

    return output_file_path


def write_output(output_file_path, output_data):
    """Write the processed template data to an output YAML file."""
    try:
        with open(output_file_path, 'w', encoding='utf-8') as file_handle:
            file_handle.write(output_data)
        logging.info("Generated manifest %s", output_file_path)
    except IOError as e:
        logging.error("Error writing to %s manifest file with %s", output_file_path, e)
        raise


def process_file(input_file, output_dir):
    """Process a single input file dynamically based on the spec."""
    try:
        input_data = load_file(input_file)

        # Dynamically get the correct spec file
        spec_file = get_spec_file(input_data)
        spec_data = load_file(spec_file)

        template_file = spec_data['cluster']['clusterDefinition']['template']
        mapping_file = "../../../Cluster-artifacts/Templates/FieldMapping.json"

        validate_paths(mapping_file, template_file)

        field_mapping = load_file(mapping_file)
        with open(template_file, 'r', encoding='utf-8') as template_handle:
            template_data = template_handle.read()

        validate_input(input_data, field_mapping)
        output_data = replace_placeholders(template_data, input_data)
        output_file_path = generate_output(input_data, output_dir)
        write_output(output_file_path, output_data)

    except FileNotFoundError as e:
        logging.error("File not found: %s. Error: %s", input_file, e)
    except json.JSONDecodeError as e:
        logging.error("JSON decode error in file %s. Error: %s", input_file, e)
    except ValueError as e:
        logging.error("Value error in file %s. Error: %s", input_file, e)
    except IOError as e:
        logging.error("I/O error in file %s. Error: %s", input_file, e)
    except Exception as e:
        logging.error("Unexpected error in file %s. Error: %s", input_file, e)
        raise


def main():
    """Main function to process multiple input files passed as arguments."""
    if len(sys.argv) < 3:
        print("Usage: python3 change.py input.json output_directory")
        sys.exit(1)

    input_files = sys.argv[1:-1]
    output_dir = sys.argv[-1]

    for input_file in input_files:
        if not os.path.exists(input_file):
            logging.error("Input file '%s' not found.", input_file)
            continue

        try:
            process_file(input_file, output_dir)
        except FileNotFoundError as e:
            logging.error("File not found: %s. Error: %s", input_file, e)
        except json.JSONDecodeError as e:
            logging.error("JSON decode error %s. Error: %s", input_file, e)
        except ValueError as e:
            logging.error("Value error in file %s. Error: %s", input_file, e)
        except IOError as e:
            logging.error("I/O error in file %s. Error: %s", input_file, e)
        except Exception as e:
            logging.error("Unexpected error file %s. Error: %s", input_file, e)
            raise


if __name__ == "__main__":
    main()

git fetch origin
git checkout $BRANCH_NAME

# Commit local changes if any
if ! git diff-index --quiet HEAD --; then
  git add .
  git commit -m "Automated commit"
fi

# Pull with rebase
git pull origin $BRANCH_NAME --rebase

# Push changes
git push origin $BRANCH_NAME



- name: Configure git
  run: |
    git config user.name "test-user"
    git config user.email "test-user@test.com"

- name: Stash untracked files (if any)
  run: git stash --include-untracked

- name: Switch to a new branch
  run: git checkout -b $BRANCH_NAME

- name: Pull changes and rebase
  run: git pull origin $BRANCH_NAME --rebase

- name: Reapply stashed changes (if applicable)
  run: git stash pop || true

- name: Commit and push changes
  env:
    ACCESS_TOKEN: ${{ secrets.ACCESS_TOKEN }}
  run: |
    git add .
    git commit -m "Automated commit"
    git push https://x-access-token:${{ secrets.ACCESS_TOKEN }}@github.com/${{ github.repository }} $BRANCH_NAME


- name: Commit and Push Changes
  env:
    ACCESS_TOKEN: ${{ env.ACCESS_TOKEN }}
  run: |
    git config user.name "usertest"
    git config user.email "usertest@test.com"
    
    # Stash any unstaged changes to prevent merge conflicts
    git stash
    
    # Checkout the branch and pull the latest changes from remote
    git checkout -b $BRANCH_NAME
    git pull origin $BRANCH_NAME --rebase
    
    # Apply stashed changes, if any
    git stash pop || true
    
    # Stage and commit the changes
    git add .
    git commit -m "Automated commit"
    
    # Push changes, use --force-with-lease to ensure safe push
    git push https://x-access-token:${ACCESS_TOKEN}@github.com/${{ github.repository }} $BRANCH_NAME --force-with-lease



PR_RESPONSE=$(curl -X POST \
    -H "Authorization: Bearer $ACCESS_TOKEN" \
    -H "Accept: application/vnd.github+json" \
    https://api.github.com/repos/ValueMomentum-Inc/GEICOPOC-Spec/pulls \
    -d '{
      "title": "Automated PR",
      "head": "'"${BRANCH_NAME}"'",
      "base": "main",
      "body": "Automated PR created via GitHub Actions"
    }')



- name: Create a Pull Request
  env:
    ACCESS_TOKEN: ${{ env.ACCESS_TOKEN }}
  run: |
    PR_RESPONSE=$(curl -X POST \
      -H "Authorization: Bearer ${ACCESS_TOKEN}" \
      -H "Accept: application/vnd.github+json" \
      https://api.github.com/repos/${{ github.repository }}/pulls \
      -d '{
        "title": "Automated PR",
        "head": "'"${{ github.actor }}/$BRANCH_NAME"'",
        "base": "main",
        "body": "Automated PR created via GitHub Actions"
      }')
    echo "Pull Request Response: $PR_RESPONSE"


- name: Debug PR Creation
  run: |
    echo "Branch: $BRANCH_NAME"
    echo "Repo: ${{ github.repository }}"
    echo "PR Response: $PR_RESPONSE"


"""
A module that handles input file validation, placeholder replacement,
and output generation.This is the Python package which generates,
the YAML manifest with the directory structure.
Author: Prasad Jivane
Date created/Modified: 11/09/2024
Usage of this module: python main.py input file paths
"""

import json
import re
import os
import sys
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)


def load_file(filepath):
    """Load and parse a JSON file."""
    try:
        with open(filepath, 'r', encoding='utf-8') as file_handle:
            return json.load(file_handle)
    except FileNotFoundError:
        logging.error("File %s not found.", filepath)
        raise
    except json.JSONDecodeError:
        logging.error("Error decoding JSON in file %s", filepath)
        raise


def validate_input(input_data, field_mapping):
    """
    Validate input fields based on the rules provided in field mapping
    (regex, min_length, max_length).
    """
    common_fields = field_mapping.get('common', {})

    for field, rules in common_fields.items():
        value = input_data.get(field)

        if not value:
            if rules.get('required', False):
                raise ValueError(
                    f"Field '{field}' is required but missing in input.json."
                )
            continue

        if not isinstance(value, str):
            raise ValueError(
                f"Field '{field}' should string. Got {type(value)} instead."
            )

        min_length = rules.get("min_length")
        if min_length and len(value) < min_length:
            raise ValueError(
                f"Field '{field}' is shorter than min. length of {min_length}."
            )

        max_length = rules.get("max_length")
        if max_length and len(value) > max_length:
            raise ValueError(
                f"Field '{field}' exceeds the maximum length of {max_length}."
            )

        pattern = rules.get("regex")
        if pattern and not re.match(pattern, value):
            raise ValueError(
                f"Field '{field}' not match required pattern '{pattern}'."
            )

    logging.info("Validation successful!")


def replace_placeholders(template_data, input_data):
    """
    Replace placeholders in the template with values from the input data.
    """
    for key, value in input_data.items():
        placeholder = f"${{{key}}}"
        template_data = template_data.replace(placeholder, value)

    return template_data


def validate_file_presence(mapping_file, template_file):
    """
    Validate that the mapping and template files exist.
    """
    if not mapping_file or not os.path.exists(mapping_file):
        raise FileNotFoundError(f"Mapping file '{mapping_file}' not found.")
    if not template_file or not os.path.exists(template_file):
        raise FileNotFoundError(f"Template file '{template_file}' not found.")


def generate_output(validated_data, output_dir):
    """
    Dynamically generate output filename based on input JSON data.
    """
    cluster_name = validated_data.get("CLUSTER_NAME")
    if not cluster_name:
        raise ValueError("CLUSTER_NAME is missing from the input data.")

    cluster_output_dir = os.path.join(output_dir, cluster_name)
    os.makedirs(cluster_output_dir, exist_ok=True)

    required_folders = ["namespaces", "config", "rbac", "network"]
    for folder in required_folders:
        destination_folder = os.path.join(cluster_output_dir, folder)
        os.makedirs(destination_folder, exist_ok=True)
        sample_file_path = os.path.join(destination_folder, "README.md")
        with open(sample_file_path, 'w', encoding='utf-8') as sample_file:
            sample_file.write(f"This is a sample file for {folder}.")

    output_file_name = f"{cluster_name}.yaml"
    output_file_path = os.path.join(cluster_output_dir, output_file_name)
    logging.info("Output file path: %s", output_file_path)

    return output_file_path


def write_output(output_file_path, output_data):
    """
    Write the processed template data to an output YAML file.
    Raises:
        IOError: If there is a problem writing to the file.
    """
    try:
        with open(output_file_path, 'w', encoding='utf-8') as file_handle:
            file_handle.write(output_data)
        logging.info("Generated manifest %s", output_file_path)
    except IOError as e:
        logging.error(
            "Error writing to %s manifest file with %s",
            output_file_path,
            e
        )
        raise


def process_file(input_file, output_dir):
    """
    Process a single input file: validation, placeholder replacement,
    and output generation.
    """
    try:
        input_data = load_file(input_file)

        mapping_file = input_data.get("mapping")
        template_file = input_data.get("template")

        validate_file_presence(mapping_file, template_file)

        field_mapping = load_file(mapping_file)
        with open(template_file, 'r', encoding='utf-8') as template_handle:
            template_data = template_handle.read()

        validate_input(input_data, field_mapping)
        output_data = replace_placeholders(template_data, input_data)
        output_file_path = generate_output(input_data, output_dir)
        write_output(output_file_path, output_data)

    except FileNotFoundError as e:
        logging.error("File not found: %s. Error: %s", input_file, e)
    except json.JSONDecodeError as e:
        logging.error("JSON decode error in file %s. Error: %s", input_file, e)
    except ValueError as e:
        logging.error("Value error in file %s. Error: %s", input_file, e)
    except IOError as e:
        logging.error("I/O error in file %s. Error: %s", input_file, e)
    except Exception as e:
        logging.error("Unexpected error file %s. Error: %s", input_file, e)
        raise


def main():
    """
    Main function to process multiple input files passed as arguments.
    """
    if len(sys.argv) < 3:
        print("Usage: python3 main.py input.json, output_directory")
        sys.exit(1)

    # input_files = sys.argv[1:]
    # output_dir = sys.argv[2:]
    input_files = sys.argv[1:-1]
    output_dir = sys.argv[-1]

    for input_file in input_files:
        if not os.path.exists(input_file):
            logging.error("Input file '%s' not found.", input_file)
            continue

        try:
            process_file(input_file, output_dir)
        except FileNotFoundError as e:
            logging.error("File not found: %s. Error: %s", input_file, e)
        except json.JSONDecodeError as e:
            logging.error("JSON decode error %s. Error: %s", input_file, e)
        except ValueError as e:
            logging.error("Value error in file %s. Error: %s", input_file, e)
        except IOError as e:
            logging.error("I/O error in file %s. Error: %s", input_file, e)
        except Exception as e:
            logging.error("Unexpected error file %s. Error: %s", input_file, e)
            raise


if __name__ == "__main__":
    main()

