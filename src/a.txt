      - name: Set up Git
        run: |
          git fetch --prune --unshallow || git fetch --prune --depth=50

      - name: Get newly added or modified files
        run: |
          # Fetch the latest commit and attempt to fetch previous commit
          new_commit=$(git rev-parse HEAD)

          # Check if it's the first commit (i.e., no previous commit)
          if git rev-parse HEAD^ > /dev/null 2>&1; then
            prev_commit=$(git rev-parse HEAD^)
          else
            echo "No previous commit found, using the initial commit"
            prev_commit=$new_commit
          fi

          # Get the list of newly added or modified json files
          new_files=$(git diff --name-only --diff-filter=AM $prev_commit $new_commit -- '*.json')

          # Check if no new or modified files are found
          if [ -z "$new_files" ]; then
            echo "No input json files found"
            exit 1
          fi

          # Extract the filename without path and extension
          branch_name=$(basename "$new_files" | sed 's/\(.*\)\..*/\1/')
          
          # Get the User information
          committer_name=$(git log -1 --pretty=format:'%an')
          committer_email=$(git log -1 --pretty=format:'%ae')

          # Output the results
          echo "::set-output name=new_files::$new_files"
          echo "::set-output name=branch_name::$branch_name"
          echo "::set-output name=committer_name::$committer_name"
          echo "::set-output name=committer_email::$committer_email"


name: Main

on:
  push:
    branches:
      - error
    paths:
      - "*/AKS/Input/*.json"

permissions:
  contents: write
  pull-requests: write
  actions: write

jobs:
  fetch_commits:
    runs-on: ubuntu-latest
    outputs:
      new_files: ${{ steps.get_files.outputs.new_files }}
      branch_name: ${{ steps.get_files.outputs.branch_name }}
      committer_name: ${{ steps.get_files.outputs.committer_name }}
      committer_email: ${{ steps.get_files.outputs.committer_email }}
      parent_folder: ${{ steps.get_folder.outputs.parent_folder }}
      
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Get newly added or modified files
        id: get_files
        run: |
          new_commit=$(git rev-parse HEAD)
          prev_commit=$(git rev-parse HEAD^)
          new_files=$(git diff --name-only --diff-filter=AM $prev_commit $new_commit -- '*.json')
          if [ -z "$new_files" ]; then
            echo "No input json files found"
            exit 1
          fi
          branch_name=$(basename "$new_files" | sed 's/\(.*\)\..*/\1/')
          committer_name=$(git log -1 --pretty=format:'%an')
          committer_email=$(git log -1 --pretty=format:'%ae')

          echo "::set-output name=new_files::$new_files"
          echo "::set-output name=branch_name::$branch_name"
          echo "::set-output name=committer_name::$committer_name"
          echo "::set-output name=committer_email::$committer_email"

      - name: Extract Parent Folder
        id: get_folder
        run: |
          parent_folder=$(basename $(dirname $(dirname $(dirname "${{ steps.get_files.outputs.new_files }}"))))
          echo "::set-output name=parent_folder::$parent_folder"

  setup_environment:
    needs: fetch_commits
    runs-on: ubuntu-latest
    outputs:
      env_file: ${{ steps.set_env.outputs.env_file }}

    steps:
      - name: Load environment variables
        id: set_env
        run: |
          case "${{ needs.fetch_commits.outputs.parent_folder }}" in
            "Clusters-Dev") env_file="./workflow-config/dev.env" ;;
            "Clusters-NonProd") env_file="./workflow-config/non-prod.env" ;;
            "Clusters-Prod") env_file="./workflow-config/prod.env" ;;
            "Clusters-sb") env_file="./workflow-config/sb.env" ;;
            *) echo "Invalid parent folder"; exit 1 ;;
          esac
          if [ -f $env_file ]; then
            echo "::set-output name=env_file::$env_file"
          else
            echo "$env_file not found!"
            exit 1
          fi

  run_script:
    needs: [fetch_commits, setup_environment]
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.9"

      - name: Cache Python dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run main script
        env:
          INPUT_FILE: ${{ needs.fetch_commits.outputs.new_files }}
          OUTPUT_DIR: ${{ needs.setup_environment.outputs.env_file }}
        run: |
          python Automation/AKS/Workload-Cluster/main.py \
            --input "${INPUT_FILE}" \
            --output "$OUTPUT_DIR" \
            --mapping "$MAPPING_PATH" \
            --template "$TEMPLATE_PATH"

  create_pr:
    needs: run_script
    runs-on: ubuntu-latest
    steps:
      - name: Generate JWT for GitHub App
        env:
          GITHUB_APP_ID: ${{ secrets.VAM_G_APP_ID }}
          GITHUB_APP_PRIVATE_KEY: ${{ secrets.VAM_G_PRIVATE_KEY }}
        run: |
          HEADER=$(echo -n '{"alg":"RS256","typ":"JWT"}' | openssl base64 | tr -d '=' | tr '/+' '_-' | tr -d '\n')
          PAYLOAD=$(echo -n '{"iat":'$(date +%s)',"exp":'$(($(date +%s) + 600))',"iss":"'"${GITHUB_APP_ID}"'"}' | openssl base64 | tr -d '=' | tr '/+' '_-' | tr -d '\n')
          echo "$GITHUB_APP_PRIVATE_KEY" > private_key.pem
          JWT_SIGNATURE=$(echo -n "${HEADER}.${PAYLOAD}" | openssl dgst -sha256 -sign private_key.pem | openssl base64 | tr -d '=' | tr '/+' '_-' | tr -d '\n')
          JWT="${HEADER}.${PAYLOAD}.${JWT_SIGNATURE}"
          echo "JWT=${JWT}" >> $GITHUB_ENV
          rm -rf private_key.pem

      - name: Get GitHub Installation Access Token
        env:
          INSTALLATION_ID: ${{ secrets.INSTALLATION_ID }}
        run: |
          INSTALLATION_TOKEN=$(curl -X POST \
            -H "Authorization: Bearer ${JWT}" \
            -H "Accept: application/vnd.github+json" \
            https://api.github.com/app/installations/${INSTALLATION_ID}/access_tokens \
            | jq -r .token)
          echo "ACCESS_TOKEN=${INSTALLATION_TOKEN}" >> $GITHUB_ENV

      - name: Create Pull Request
        run: |
          curl -X POST \
            -H "Authorization: Bearer ${{ secrets.ACCESS_TOKEN }}" \
            -H "Accept: application/vnd.github+json" \
            https://api.github.com/repos/${{ github.repository }}/pulls \
            -d '{
              "title": "Automated PR",
              "head": "'"${{ needs.fetch_commits.outputs.branch_name }}"'",
              "base": "'"${{ github.ref_name }}"'",
              "body": "Automated PR created by GitHub Actions"
            }'



name: v1

on:
  push:
    branches:
      - error
    paths:
      - "*/AKS/Input/*.json"

permissions:
  contents: write
  pull-requests: write
  actions: write

jobs:
  trigger_on:
    runs-on: ubuntu-latest
    container: # Custom container with pre-installed Python and jq
      image: ghcr.io/your-organization/custom-image:python-jq

    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        with:
          fetch-depth: 1 # Shallow fetch for faster execution

      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Get newly added or modified files
        run: |
          new_commit=$(git rev-parse HEAD)
          prev_commit=$(git rev-parse HEAD^)

          new_files=$(git diff --name-only --diff-filter=AM $prev_commit $new_commit -- '*.json')
          echo "NEW_FILES=$new_files" >> $GITHUB_ENV

          branch_name=$(basename "$new_files" | sed 's/\(.*\)\..*/\1/')
          echo "BRANCH_NAME=$branch_name" >> $GITHUB_ENV

          if [ -z "$new_files" ]; then
            echo "No input json files found"
            exit 1
          fi

          committer_name=$(git log -1 --pretty=format:'%an')
          committer_email=$(git log -1 --pretty=format:'%ae')
          echo "COMMITTER_NAME=$committer_name" >> $GITHUB_ENV
          echo "COMMITTER_EMAIL=$committer_email" >> $GITHUB_ENV

      - name: Extract Parent Folder
        run: |
          parent_folder=$(basename $(dirname $(dirname $(dirname "$NEW_FILES"))))
          echo "BASE_FOLDER=$parent_folder" >> $GITHUB_ENV

      - name: Choose Environment and Load Variables
        run: |
          case "$BASE_FOLDER" in
            "Clusters-Dev") ENV_FILE="./workflow-config/dev.env" ;;
            "Clusters-NonProd") ENV_FILE="./workflow-config/non-prod.env" ;;
            "Clusters-Prod") ENV_FILE="./workflow-config/prod.env" ;;
            "Clusters-sb") ENV_FILE="./workflow-config/sb.env" ;;
            *) echo "Invalid parent folder: $BASE_FOLDER"; exit 1 ;;
          esac

          if [ -f $ENV_FILE ]; then
            while IFS= read -r line; do
              [[ ! $line =~ ^# && $line =~ = ]] && echo "$line" >> $GITHUB_ENV
            done < $ENV_FILE
          else
            echo "$ENV_FILE not found!"
            exit 1
          fi

      - name: Set environment-specific secrets
        run: |
          case "$BASE_FOLDER" in
            "Clusters-Dev")
              echo "ARGOPWD=${{ secrets.DEV_ARGOPWD }}" >> $GITHUB_ENV
              echo "GITHUB_APP_ID=${{ secrets.VAM_G_APP_ID }}" >> $GITHUB_ENV
              echo "INSTALLATION_ID=${{ secrets.INSTALLATION_ID }}" >> $GITHUB_ENV
              echo "AZURE_CLIENT_SECRET=${{ secrets.DV1_AZURE_CLIENT_SECRET }}" >> $GITHUB_ENV
              ;;
            "Clusters-NonProd")
              echo "ARGOPWD=${{ secrets.NP_ARGOPWD }}" >> $GITHUB_ENV
              echo "GITHUB_APP_ID=${{ secrets.GAPP_ID }}" >> $GITHUB_ENV
              echo "INSTALLATION_ID=${{ secrets.GINSTALLATION_ID }}" >> $GITHUB_ENV
              echo "AZURE_CLIENT_SECRET=${{ secrets.NP_AZURE_CLIENT_SECRET }}" >> $GITHUB_ENV
              ;;
            "Clusters-Prod")
              echo "ARGOPWD=${{ secrets.PROD_ARGOPWD }}" >> $GITHUB_ENV
              echo "GITHUB_APP_ID=${{ secrets.GAPP_ID_PROD }}" >> $GITHUB_ENV
              echo "INSTALLATION_ID=${{ secrets.GINSTALLATION_ID_PROD }}" >> $GITHUB_ENV
              echo "AZURE_CLIENT_SECRET=${{ secrets.PD_AZURE_CLIENT_SECRET }}" >> $GITHUB_ENV
              ;;
            "Clusters-sb")
              echo "ARGOPWD=${{ secrets.SB_ARGOPWD }}" >> $GITHUB_ENV
              echo "GITHUB_APP_ID=${{ secrets.GAPP_ID }}" >> $GITHUB_ENV
              echo "INSTALLATION_ID=${{ secrets.GINSTALLATION_ID }}" >> $GITHUB_ENV
              echo "AZURE_CLIENT_SECRET=${{ secrets.SB_AZURE_CLIENT_SECRET }}" >> $GITHUB_ENV
              ;;
            *) echo "Error: Unknown Environment. Exiting."; exit 1 ;;
          esac

      - name: Run main script
        run: |
          python Automation/AKS/Workload-Cluster/main.py \
            --input "${NEW_FILES}" \
            --output "$OUTPUT_DIR" \
            --mapping "$MAPPING_PATH" \
            --template "$TEMPLATE_PATH"

      - name: Generate JWT for GitHub App
        run: |
          HEADER=$(echo -n '{"alg":"RS256","typ":"JWT"}' | openssl base64 | tr -d '=' | tr '/+' '_-' | tr -d '\n')
          PAYLOAD=$(echo -n '{"iat":'$(date +%s)',"exp":'$(($(date +%s) + 600))',"iss":"'"${GITHUB_APP_ID}"'"}' | openssl base64 | tr -d '=' | tr '/+' '_-' | tr -d '\n')
          echo "$GITHUB_APP_PRIVATE_KEY" > private_key.pem
          JWT_SIGNATURE=$(echo -n "${HEADER}.${PAYLOAD}" | openssl dgst -sha256 -sign private_key.pem | openssl base64 | tr -d '=' | tr '/+' '_-' | tr -d '\n')
          JWT="${HEADER}.${PAYLOAD}.${JWT_SIGNATURE}"
          echo "JWT=${JWT}" >> $GITHUB_ENV

      - name: Get GitHub Installation Access Token
        run: |
          INSTALLATION_TOKEN=$(curl -X POST \
            -H "Authorization: Bearer ${JWT}" \
            -H "Accept: application/vnd.github+json" \
            https://api.github.com/app/installations/${INSTALLATION_ID}/access_tokens \
            | jq -r .token)
          echo "ACCESS_TOKEN=${INSTALLATION_TOKEN}" >> $GITHUB_ENV

      - name: Configure git
        run: |
          git config user.name "$COMMITTER_NAME"
          git config user.email "$COMMITTER_EMAIL"

      - name: Switch to a new branch and push changes
        run: |
          git checkout -b $BRANCH_NAME
          git pull origin $BRANCH_NAME --rebase || true
          git add .
          git commit -m "Automated commit by $COMMITTER_NAME"
          git push https://x-access-token:${{ secrets.ACCESS_TOKEN }}@github.com/${{ github.repository }} $BRANCH_NAME

      - name: Create a Pull Request
        run: |
          PR_RESPONSE=$(curl -X POST \
            -H "Authorization: Bearer $ACCESS_TOKEN" \
            -H "Accept: application/vnd.github+json" \
            https://api.github.com/repos/${{ github.repository }}/pulls \
            -d '{
              "title": "Automated PR",
              "head": "'"${BRANCH_NAME}"'",
              "base": "'"${{ github.ref_name }}"'",
              "body": "Automated PR created by GITHUB workflow"
            }')
          echo "PR Response: $PR_RESPONSE"






import os
import json
import copy
import argparse
import logging
import re
from dotenv import load_dotenv
import yaml
from pathlib import Path

# Load environment variables from .env file
load_dotenv()

# Get paths from environment variables
mapping_file = os.getenv("MAPPING_PATH")
template_file = os.getenv("TEMPLATE_PATH")
output_dir = os.getenv("OUTPUT_DIR")

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


def load_json(file_path: str) -> dict:
    """Loads and strips JSON data from a file."""
    try:
        with open(file_path, "r", encoding="utf-8") as file:
            # Ensure all values are stripped of leading/trailing whitespace
            data = json.load(file)
            return {k: v.strip() if isinstance(v, str) else v for k, v in data.items()}
    except (FileNotFoundError, json.JSONDecodeError) as e:
        logging.error("Error loading %s JSON file: %s", file_path, e)
        raise


def validate_field(key, value, properties):
    """Validates field based on mapping properties."""
    if properties["required"] and (value is None or value.strip() == ""):
        raise ValueError(f"Mandatory field '{key}' is missing.")

    # Apply strip() to value if it's a string
    value = value.strip() if isinstance(value, str) else value

    # Regex validation
    if "regex" in properties and not re.match(properties["regex"], value):
        raise ValueError(f"Field '{key}' does not match the pattern {properties['regex']}")

    # Length validation
    if "min_length" in properties and len(value) < properties["min_length"]:
        raise ValueError(f"Field '{key}' is shorter than minimum length {properties['min_length']}")
    if "max_length" in properties and len(value) > properties["max_length"]:
        raise ValueError(f"Field '{key}' exceeds maximum length {properties['max_length']}")

    return value


def validate_and_apply_defaults(mapping: dict, inputs: dict) -> tuple:
    """Validates input values based on the mapping and applies defaults."""
    validated_data = {}
    
    logging.debug("Validating common fields...")
    
    for key, properties in mapping.get("common", {}).items():
        value = inputs.get(key, properties.get("default"))
        validated_data[key] = validate_field(key, value, properties)

    manifests_data = {}
    for manifest, fields in mapping.get("manifests", {}).items():
        manifest_data = {}
        for key, properties in fields.items():
            value = inputs.get(key, properties.get("default"))
            manifest_data[key] = validate_field(key, value, properties)
        manifests_data.update(manifest_data)

    logging.info("Input validation successful.")
    return validated_data, manifests_data


def replace_placeholders(yaml_content, replacements):
    """Replaces placeholders in the YAML content with actual values."""
    if isinstance(yaml_content, dict):
        return {k: replace_placeholders(v, replacements) for k, v in yaml_content.items()}
    if isinstance(yaml_content, list):
        return [replace_placeholders(i, replacements) for i in yaml_content]
    if isinstance(yaml_content, str):
        for key, value in replacements.items():
            placeholder = f"${{{key}}}"
            if placeholder in yaml_content:
                logging.debug(f"Replacing {placeholder} with {value}")
                yaml_content = yaml_content.replace(placeholder, str(value))
        return yaml_content
    return yaml_content


def update_yaml_manifest(yaml_template: str, validated_data: dict, manifests_data: dict) -> str:
    """Updates the YAML manifest with the validated input values."""
    yaml_docs = yaml.safe_load_all(yaml_template)
    updated_docs = []

    for doc in yaml_docs:
        updated_doc = copy.deepcopy(doc)
        replacements = {**validated_data, **manifests_data}
        updated_doc = replace_placeholders(updated_doc, replacements)
        updated_docs.append(updated_doc)

    logging.info("YAML manifest successfully updated with input values.")
    return "---\n".join([yaml.safe_dump(doc) for doc in updated_docs])


def generate_output(validated_data, output):
    """Dynamically generate output directory and filename based on input JSON data."""
    cluster_name = validated_data.get("CLUSTER_NAME")
    base_output_dir = Path(output)

    output_dirs = base_output_dir / f"{cluster_name}"
    output_dirs.mkdir(parents=True, exist_ok=True)

    # Define required subdirectories
    required_folders = ["namespaces", "config", "rbac", "network"]

    for folder in required_folders:
        destination_folder = output_dirs / folder
        destination_folder.mkdir(exist_ok=True)

        sample_file_path = destination_folder / "README.md"
        with sample_file_path.open("w", encoding="utf-8") as sample_file:
            sample_file.write(f"This is a sample file for {folder}.")

    output_file_name = f"{cluster_name}.yaml"
    return output_dirs / output_file_name


def handle_arguments():
    """Handle command-line arguments."""
    parser = argparse.ArgumentParser(
        description="Generate Kubernetes cluster definitions from templates."
    )
    parser.add_argument("--input", required=True, help="Path to the input JSON file")
    parser.add_argument("--output", required=True, help="Path to the output directory")
    parser.add_argument("--mapping", required=True, help="Path to the mapping JSON file")
    parser.add_argument("--template", required=True, help="Path to the YAML template file")

    return parser.parse_args()


def main():
    """Main function to run the program."""
    args = handle_arguments()

    try:
        # Ensure input, mapping, and template paths exist
        for path in [args.input, args.mapping, args.template]:
            if not os.path.isfile(path):
                raise FileNotFoundError(f"File '{path}' does not exist.")

        # Load files and validate data
        logging.info("Loading input, mapping, and template files...")
        mapping = load_json(args.mapping)
        inputs = load_json(args.input)
        validated_data, manifests_data = validate_and_apply_defaults(mapping, inputs)

        with open(args.template, "r", encoding="utf-8") as yaml_file:
            yaml_template = yaml_file.read()

        updated_yaml = update_yaml_manifest(yaml_template, validated_data, manifests_data)

        # Generate output
        output_file_path = generate_output(validated_data, args.output)

        # Save the updated YAML
        with open(output_file_path, "w", encoding="utf-8") as updated_yaml_file:
            updated_yaml_file.write(updated_yaml)

        logging.info("Cluster definition saved to '%s'.", output_file_path)

    except FileNotFoundError as fnf_error:
        logging.error(fnf_error)
    except ValueError as val_error:
        logging.error("Validation error: %s", val_error)
    except Exception as e:
        logging.error("An unexpected error occurred: %s", e)
        raise


if __name__ == "__main__":
    main()



"CLUSTER_LOCATION": {
    "required": true,
    "regex": "^[a-zA-Z0-9-]+$",
    "min_length": 3,
    "max_length": 20
  }

import os
import json
import copy
import argparse
import logging
import re
from dotenv import load_dotenv
import yaml

# Load environment variables from .env file
load_dotenv()

# Get paths from environment variables
input_file = os.getenv("INPUT_PATH")
mapping_file = os.getenv("MAPPING_PATH")
template_file = os.getenv("TEMPLATE_PATH")
output_dir = os.getenv("OUTPUT_DIR")

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


def load_json(file_path: str) -> dict:
    """Loads and strips JSON data from a file."""
    try:
        with open(file_path, "r", encoding="utf-8") as file:
            # Ensure all values are stripped of leading/trailing whitespace
            data = json.load(file)
            return {k: v.strip() if isinstance(v, str) else v for k, v in data.items()}
    except (FileNotFoundError, json.JSONDecodeError) as e:
        logging.error("Error loading %s JSON file: %s", file_path, e)
        raise


def validate_field(key, value, properties):
    """Validates field based on mapping properties."""
    if properties["required"] and (value is None or value == ""):
        raise ValueError(f"Mandatory field '{key}' is missing.")
    
    # Apply strip() to value if it's a string
    value = value.strip() if isinstance(value, str) else value

    # Regex validation
    if "regex" in properties and not re.match(properties["regex"], value):
        raise ValueError(f"Field '{key}' does not match the pattern {properties['regex']}")

    # Length validation
    if "min_length" in properties and len(value) < properties["min_length"]:
        raise ValueError(f"Field '{key}' is shorter than minimum length {properties['min_length']}")
    if "max_length" in properties and len(value) > properties["max_length"]:
        raise ValueError(f"Field '{key}' exceeds maximum length {properties['max_length']}")
    
    return value


def validate_and_apply_defaults(mapping: dict, inputs: dict) -> tuple:
    """Validates input values based on the mapping and applies defaults."""
    validated_data = {}

    # Validate and apply defaults for common fields
    for key, properties in mapping.get("common", {}).items():
        value = inputs.get(key, properties.get("default"))
        validated_data[key] = validate_field(key, value, properties)

    # Validate and apply defaults for manifest-specific fields
    manifests_data = {}
    for manifest, fields in mapping.get("manifests", {}).items():
        manifest_data = {}
        for key, properties in fields.items():
            value = inputs.get(key, properties.get("default"))
            manifest_data[key] = validate_field(key, value, properties)
        manifests_data.update(manifest_data)

    return validated_data, manifests_data


def replace_placeholders(yaml_content, replacements):
    """Replaces placeholders in the YAML content with actual values."""
    if isinstance(yaml_content, dict):
        return {
            k: replace_placeholders(v, replacements) for k, v in yaml_content.items()
        }
    if isinstance(yaml_content, list):
        return [replace_placeholders(i, replacements) for i in yaml_content]
    if isinstance(yaml_content, str):
        for key, value in replacements.items():
            placeholder = f"${{{key}}}"
            if placeholder in yaml_content:
                yaml_content = yaml_content.replace(placeholder, str(value))
        return yaml_content
    return yaml_content


def update_yaml_manifest(yaml_template: str, validated_data: dict, manifests_data: dict) -> str:
    """Updates the YAML manifest with the validated input values."""
    yaml_docs = yaml.safe_load_all(yaml_template)

    updated_docs = []

    for doc in yaml_docs:
        updated_doc = copy.deepcopy(doc)

        # Combine validated_data and manifest-specific data
        replacements = {**validated_data, **manifests_data}

        updated_doc = replace_placeholders(updated_doc, replacements)
        updated_docs.append(updated_doc)

    return "\n---\n".join([yaml.safe_dump(doc) for doc in updated_docs])


def generate_output(validated_data, output):
    """Dynamically generate output directory and filename based on input JSON data."""
    cluster_name = validated_data.get("CLUSTER_NAME")
    base_output_dir = output

    output_dirs = os.path.join(base_output_dir, f"{cluster_name}")
    os.makedirs(output_dirs, exist_ok=True)

    # Define required subdirectories
    required_folders = ["namespaces", "config", "rbac", "network"]

    for folder in required_folders:
        destination_folder = os.path.join(output_dirs, folder)
        os.makedirs(destination_folder, exist_ok=True)

        sample_file_path = os.path.join(destination_folder, "README.md")
        with open(sample_file_path, "w", encoding="utf-8") as sample_file:
            sample_file.write(f"This is a sample file for {folder}.")

    output_file_name = f"{cluster_name}.yaml"
    return os.path.join(output_dirs, output_file_name)


def main():
    """Main function to run the program."""
    parser = argparse.ArgumentParser(
        description="Generate Kubernetes cluster definitions from templates."
    )
    parser.add_argument("--input", required=True, help="Path to the input JSON file")
    parser.add_argument("--output", required=True, help="Path to the output directory")
    parser.add_argument("--mapping", required=True, help="Path to the mapping JSON file")
    parser.add_argument("--template", required=True, help="Path to the YAML template file")

    args = parser.parse_args()

    try:
        mapping = load_json(args.mapping)
        inputs = load_json(args.input)
        output = args.output

        validated_data, manifests_data = validate_and_apply_defaults(mapping, inputs)

        with open(args.template, "r", encoding="utf-8") as yaml_file:
            yaml_template = yaml_file.read()

        updated_yaml = update_yaml_manifest(yaml_template, validated_data, manifests_data)

        output_file_path = generate_output(validated_data, output)

        with open(output_file_path, "w", encoding="utf-8") as updated_yaml_file:
            updated_yaml_file.write(updated_yaml)

        logging.info("Cluster definition saved to '%s'.", output_file_path)
    
    except Exception as e:
        logging.error("An error occurred: %s", e)
        raise


if __name__ == "__main__":
    main()


import json
import os
import sys
import logging
import re
from dotenv import load_dotenv


# Load environment variables from .env file
load_dotenv()

# Get paths from environment variables
mapping_file = os.getenv("MAPPING_PATH")
output_dir_global = os.getenv("OUTPUT_DIR")  # renamed to avoid shadowing
spec_file_template = os.getenv("SPEC_PATH")  # template with {spec_type} placeholder

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)


def load_file(filepath):
    """Load and parse a JSON file."""
    try:
        with open(filepath, 'r', encoding='utf-8') as file_handle:
            return json.load(file_handle)
    except FileNotFoundError:
        logging.error("File %s not found.", filepath)
        raise
    except json.JSONDecodeError:
        logging.error("Error decoding JSON in file %s", filepath)
        raise


def get_spec_file(input_data):
    """Get the spec file path based on the input data spec field."""
    spec_type = input_data.get("spec", "shared")  # Default to "shared" if not provided
    spec_file = spec_file_template.replace("{spec_type}", spec_type)

    if not os.path.exists(spec_file):
        raise FileNotFoundError(f"Spec file '{spec_file}' not found for spec type: {spec_type}")
    return spec_file


def validate_input(input_data, field_mapping):
    """Validate input fields based on the rules provided in field mapping."""
    common_fields = field_mapping.get('common', {})

    for field, rules in common_fields.items():
        value = input_data.get(field)

        if not value:
            if rules.get('required', False):
                raise ValueError(
                    f"Field '{field}' is required but missing in input.json."
                )
            continue

        if not isinstance(value, str):
            raise ValueError(
                f"Field '{field}' should be a string. Got {type(value)} instead."
            )

        min_length = rules.get("min_length")
        if min_length and len(value) < min_length:
            raise ValueError(
                f"Field '{field}' is shorter than min. length of {min_length}."
            )

        max_length = rules.get("max_length")
        if max_length and len(value) > max_length:
            raise ValueError(
                f"Field '{field}' exceeds the maximum length of {max_length}."
            )

        pattern = rules.get("regex")
        if pattern and not re.match(pattern, value):
            raise ValueError(
                f"Field '{field}' does not match required pattern '{pattern}'."
            )

    logging.info("Validation successful!")


def replace_placeholders(template_data, input_data):
    """Replace placeholders in the template with values from the input data."""
    for key, value in input_data.items():
        placeholder = f"${{{key}}}"
        template_data = template_data.replace(placeholder, value)

    return template_data


def validate_paths(*paths):
    """Validate that all provided paths exist."""
    for path in paths:
        if not os.path.exists(path):
            raise FileNotFoundError(f"Path '{path}' not found.")


def create_required_folders(base_dir, folders):
    """Create required folders inside a base directory."""
    for folder in folders:
        destination_folder = os.path.join(base_dir, folder)
        os.makedirs(destination_folder, exist_ok=True)
        sample_file_path = os.path.join(destination_folder, "README.md")
        with open(sample_file_path, 'w', encoding='utf-8') as sample_file:
            sample_file.write(f"This is a sample file for {folder}.")


def generate_output(validated_data, output_dir):
    """Dynamically generate output filename and folder structure."""
    cluster_name = validated_data.get("CLUSTER_NAME")
    if not cluster_name:
        raise ValueError("CLUSTER_NAME is missing from the input data.")

    cluster_output_dir = os.path.join(output_dir, cluster_name)
    os.makedirs(cluster_output_dir, exist_ok=True)

    create_required_folders(cluster_output_dir, ["namespaces", "config", "rbac", "network"])

    output_file_name = f"{cluster_name}.yaml"
    output_file_path = os.path.join(cluster_output_dir, output_file_name)
    logging.info("Output file path: %s", output_file_path)

    return output_file_path


def write_output(output_file_path, output_data):
    """Write the processed template data to an output YAML file."""
    try:
        with open(output_file_path, 'w', encoding='utf-8') as file_handle:
            file_handle.write(output_data)
        logging.info("Generated manifest %s", output_file_path)
    except IOError as e:
        logging.error("Error writing to %s manifest file with %s", output_file_path, e)
        raise


def process_file(input_file, output_dir):
    """Process a single input file dynamically based on the spec."""
    try:
        input_data = load_file(input_file)

        # Dynamically get the correct spec file
        spec_file_path = get_spec_file(input_data)
        spec_data = load_file(spec_file_path)

        template_file = spec_data['cluster']['clusterDefinition']['template']
        validate_paths(mapping_file, template_file)

        field_mapping = load_file(mapping_file)
        with open(template_file, 'r', encoding='utf-8') as template_handle:
            template_data = template_handle.read()

        validate_input(input_data, field_mapping)
        output_data = replace_placeholders(template_data, input_data)
        output_file_path = generate_output(input_data, output_dir)
        write_output(output_file_path, output_data)

    except FileNotFoundError as e:
        logging.error("File not found: %s. Error: %s", input_file, e)
    except json.JSONDecodeError as e:
        logging.error("JSON decode error in file %s. Error: %s", input_file, e)
    except ValueError as e:
        logging.error("Value error in file %s. Error: %s", input_file, e)
    except IOError as e:
        logging.error("I/O error in file %s. Error: %s", input_file, e)
    except Exception as e:
        logging.error("Unexpected error in file %s. Error: %s", input_file, e)
        raise


def main():
    """Main function to process multiple input files passed as arguments."""
    if len(sys.argv) < 3:
        print("Usage: python3 main.py input.json output_directory")
        sys.exit(1)

    input_files = sys.argv[1:-1]
    output_dir = sys.argv[-1]

    for input_file in input_files:
        if not os.path.exists(input_file):
            logging.error("Input file '%s' not found.", input_file)
            continue

        try:
            process_file(input_file, output_dir)
        except FileNotFoundError as e:
            logging.error("File not found: %s. Error: %s", input_file, e)
        except json.JSONDecodeError as e:
            logging.error("JSON decode error %s. Error: %s", input_file, e)
        except ValueError as e:
            logging.error("Value error in file %s. Error: %s", input_file, e)
        except IOError as e:
            logging.error("I/O error in file %s. Error: %s", input_file, e)
        except Exception as e:
            logging.error("Unexpected error file %s. Error: %s", input_file, e)
            raise


if __name__ == "__main__":
    main()



- name: Set up yq
        run: |
          sudo wget https://github.com/mikefarah/yq/releases/download/v4.16.2/yq_linux_amd64 -O /usr/bin/yq
          sudo chmod +x /usr/bin/yq

graph TD
    A[Start: Push to feature/dv1 branch] --> B[Checkout code]

    subgraph Setup Environment
        B --> C[Set up jq]
        C --> D[Set up Python 3.9]
        D --> E[Cache Python dependencies]
        E --> F[Install dependencies]
        F --> G[Set up Git]
    end

    G --> H[Get newly added or modified JSON files]

    subgraph Prepare Inputs
        H --> I[Extract Parent Folder]
        I --> J[Choose Environment and Load Variables]
        J --> K[Set environment-specific secrets]
        K --> L[List and Process JSON Files]
    end

    L --> M[Run main Python script]

    subgraph Git Operations
        M --> N[Generate JWT for GitHub App]
        N --> O[Get GitHub Installation Access Token]
        O --> P[Configure git]
        P --> Q[Stash untracked files]
        Q --> R[Switch to new branch]
        R --> S[Pull changes and rebase]
        S --> T[Reapply stashed changes]
        T --> U[Commit and push changes]
    end

    U --> V[Install ArgoCD CLI]

    subgraph ArgoCD Operations
        V --> W[Login to ArgoCD]
        W --> X[Create ArgoCD app for workload cluster]
        X --> Y[Sync ArgoCD app]
        Y --> Z[Install kubectl]
        Z --> AA[Configure kubectl]
        AA --> AB[Install clusterctl]
        AB --> AC[Verify workload cluster is provisioned]
        AC --> AD[Attach ACR to AKS cluster]
        AD --> AE[Add cluster to ArgoCD]
        AE --> AF[Verify ArgoCD apps are synced]
    end

    AF --> AG[End]


Descriptions of Each Step:
Push to feature/XYZ branch: The developer pushes code to the feature branch, which triggers the pipeline.
Checkout code: The GitHub Action checks out the latest code from the repository.
Set up jq: jq is installed to handle JSON file processing in the next steps.
Set up Python: Python is set up to run Python scripts in the pipeline.
Cache Python dependencies: Dependencies from previous runs are cached to reduce build time.
Install dependencies: Any remaining or new dependencies are installed.
Set up Git: Git is configured to allow later commits and pushes.
Get newly added or modified JSON files: Identify new or modified input.json files to be processed.
Extract Parent Folder: Extract the folder from the JSON files to prepare for environment-specific processing.
Choose Environment and Load Variables: Based on the environment (dev/prod), load the corresponding variables.
Set environment-specific secrets: Load environment-specific secrets and sensitive data.
List and Process JSON Files: List the JSON files that will be processed and validate them.
Run main Python script: The main Python script is executed, generating the Kubernetes manifests.
Generate JWT for GitHub App: A JWT is generated for authenticating with GitHub as a GitHub App.
Get GitHub Installation Access Token: Use the JWT to get an installation access token for interacting with GitHub.
Configure Git: Git is configured with the required credentials for committing and pushing changes.
Stash untracked files: Temporarily stash any untracked files to avoid conflicts when switching branches.
Switch to a new branch: Switch to a new feature branch where the changes will be pushed.
Pull changes and rebase: Pull changes from the remote repository and rebase the current branch to avoid conflicts.
Reapply stashed changes: Reapply the stashed changes to the new branch.
Commit and push changes: Commit and push the changes (including manifests) to the new branch.
Install ArgoCD CLI: Install the ArgoCD CLI for managing ArgoCD operations.
Login to ArgoCD: Log into ArgoCD to manage applications and clusters.
Create an ArgoCD app for the workload cluster: Create an ArgoCD application for deploying the workload cluster.
Sync ArgoCD app: Sync the ArgoCD app to ensure that the workload cluster is deployed correctly.
Install kubectl: Install the kubectl CLI to interact with the Kubernetes cluster.
Configure kubectl: Configure kubectl with credentials to manage the cluster.
Install clusterctl: Install the clusterctl CLI for managing Kubernetes clusters.
Verify workload cluster is provisioned: Verify that the workload cluster has been successfully provisioned.
Attach ACR to AKS cluster: Attach the Azure Container Registry (ACR) to the AKS cluster for pulling container images.
Add a cluster to ArgoCD: Add the new AKS cluster to ArgoCD for application management.
Verify that ArgoCD apps are synced: Ensure that all ArgoCD applications are synced with the current configuration.
End: The process ends after successful deployment and verification.


echo "${{ secrets.DEV_GITHUB_APP_PRIVATE_KEY }}" > private_key.pem
      echo "GITHUB_APP_PRIVATE_KEY_FILE=private_key.pem" >> $GITHUB_ENV

- name: Set environment-specific secrets
  run: |
    # Load environment-specific secrets based on the parent folder
    if [ "$BASE_FOLDER" = "Clusters-Dev" ]; then
      echo "ARGOPWD=${{ secrets.DEV_ARGOPWD }}" >> $GITHUB_ENV
      echo "GITHUB_APP_ID=${{ secrets.VAM_G_APP_ID_DEV }}" >> $GITHUB_ENV
      echo "INSTALLATION_ID=${{ secrets.INSTALLATION_ID_DEV }}" >> $GITHUB_ENV
      echo "GITHUB_APP_PRIVATE_KEY=${{ secrets.VAM_G_PRIVATE_KEY_DEV }}" >> $GITHUB_ENV
    elif [ "$BASE_FOLDER" = "Clusters-NonProd" ]; then
      echo "ARGOPWD=${{ secrets.NONPROD_ARGOPWD }}" >> $GITHUB_ENV
      echo "GITHUB_APP_ID=${{ secrets.VAM_G_APP_ID_NONPROD }}" >> $GITHUB_ENV
      echo "INSTALLATION_ID=${{ secrets.INSTALLATION_ID_NONPROD }}" >> $GITHUB_ENV
      echo "GITHUB_APP_PRIVATE_KEY=${{ secrets.VAM_G_PRIVATE_KEY_NONPROD }}" >> $GITHUB_ENV
    elif [ "$BASE_FOLDER" = "Clusters-Prod" ]; then
      echo "ARGOPWD=${{ secrets.PROD_ARGOPWD }}" >> $GITHUB_ENV
      echo "GITHUB_APP_ID=${{ secrets.VAM_G_APP_ID_PROD }}" >> $GITHUB_ENV
      echo "INSTALLATION_ID=${{ secrets.INSTALLATION_ID_PROD }}" >> $GITHUB_ENV
      echo "GITHUB_APP_PRIVATE_KEY=${{ secrets.VAM_G_PRIVATE_KEY_PROD }}" >> $GITHUB_ENV
    elif [ "$BASE_FOLDER" = "Clusters-sb" ]; then
      echo "ARGOPWD=${{ secrets.SB_ARGOPWD }}" >> $GITHUB_ENV
      echo "GITHUB_APP_ID=${{ secrets.VAM_G_APP_ID_SB }}" >> $GITHUB_ENV
      echo "INSTALLATION_ID=${{ secrets.INSTALLATION_ID_SB }}" >> $GITHUB_ENV
      echo "GITHUB_APP_PRIVATE_KEY=${{ secrets.VAM_G_PRIVATE_KEY_SB }}" >> $GITHUB_ENV
    else
      echo "Invalid environment"
      exit 1
    fi
  shell: bash




- name: Load Secrets for Specific Environment
        run: |
          # Based on the parent folder, load the corresponding environment-specific secrets
          if [ "$BASE_FOLDER" = "Clusters-Dev" ]; then
            echo "ARGOPWD=${{ secrets.DEV_ARGOPWD }}" >> $GITHUB_ENV
            echo "KUBECONF=${{ secrets.DEV_KUBECONF }}" >> $GITHUB_ENV
            echo "GITHUB_APP_ID=${{ secrets.VAM_G_APP_ID }}" >> $GITHUB_ENV
            echo "INSTALLATION_ID=${{ secrets.INSTALLATION_ID }}" >> $GITHUB_ENV
            echo "GITHUB_APP_PRIVATE_KEY=${{ secrets.VAM_G_PRIVATE_KEY }}" >> $GITHUB_ENV
          elif [ "$BASE_FOLDER" = "Clusters-NonProd" ]; then
            echo "ARGOPWD=${{ secrets.NONPROD_ARGOPWD }}" >> $GITHUB_ENV
            echo "KUBECONF=${{ secrets.NONPROD_KUBECONF }}" >> $GITHUB_ENV
            echo "GITHUB_APP_ID=${{ secrets.VAM_NONPROD_G_APP_ID }}" >> $GITHUB_ENV
            echo "INSTALLATION_ID=${{ secrets.NONPROD_INSTALLATION_ID }}" >> $GITHUB_ENV
            echo "GITHUB_APP_PRIVATE_KEY=${{ secrets.VAM_NONPROD_G_PRIVATE_KEY }}" >> $GITHUB_ENV
          elif [ "$BASE_FOLDER" = "Clusters-Prod" ]; then
            echo "ARGOPWD=${{ secrets.PROD_ARGOPWD }}" >> $GITHUB_ENV
            echo "KUBECONF=${{ secrets.PROD_KUBECONF }}" >> $GITHUB_ENV
            echo "GITHUB_APP_ID=${{ secrets.VAM_PROD_G_APP_ID }}" >> $GITHUB_ENV
            echo "INSTALLATION_ID=${{ secrets.PROD_INSTALLATION_ID }}" >> $GITHUB_ENV
            echo "GITHUB_APP_PRIVATE_KEY=${{ secrets.VAM_PROD_G_PRIVATE_KEY }}" >> $GITHUB_ENV
          elif [ "$BASE_FOLDER" = "Clusters-sb" ]; then
            echo "ARGOPWD=${{ secrets.SB_ARGOPWD }}" >> $GITHUB_ENV
            echo "KUBECONF=${{ secrets.SB_KUBECONF }}" >> $GITHUB_ENV
            echo "GITHUB_APP_ID=${{ secrets.VAM_SB_G_APP_ID }}" >> $GITHUB_ENV
            echo "INSTALLATION_ID=${{ secrets.SB_INSTALLATION_ID }}" >> $GITHUB_ENV
            echo "GITHUB_APP_PRIVATE_KEY=${{ secrets.VAM_SB_G_PRIVATE_KEY }}" >> $GITHUB_ENV
          else
            echo "Invalid environment"
            exit 1
          fi


echo "OUTPUT_DIR=$OUTPUT_DIR" >> $GITHUB_ENV
echo "MAPPING_PATH=$MAPPING_PATH" >> $GITHUB_ENV
echo "TEMPLATE_PATH=$TEMPLATE_PATH" >> $GITHUB_ENV

run: |
  echo "Running Python script with the following arguments:"
  echo "Input: ${NEW_FILES}"
  echo "Output: $OUTPUT_DIR"
  echo "Mapping: $MAPPING_PATH"
  echo "Template: $TEMPLATE_PATH"
  
  python Automation/AKS/Workload-Cluster/main.py \
    --input "${NEW_FILES}" \
    --output "$OUTPUT_DIR" \
    --mapping "$MAPPING_PATH" \
    --template "$TEMPLATE_PATH"


echo "OUTPUT_DIR: $OUTPUT_DIR"
echo "MAPPING_PATH: $MAPPING_PATH"
echo "TEMPLATE_PATH: $TEMPLATE_PATH"


name: mainv2

on:
  push:
    branches:
      - newaddons
    paths:
      - "Clusters-Dev/AKS/Workload/**/*.yaml"   # Triggers on added or modified YAML files

env:
  MANAGEMENT_CLUSTER_KUBECONF: ${{ secrets.KUBECONFIG }}

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Git
        run: git fetch --prune --unshallow

      - name: Get newly added or modified YAML files
        run: |
          # Fetch the latest commit and the previous commit
          new_commit=$(git rev-parse HEAD)
          prev_commit=$(git rev-parse HEAD^)

          # List YAML files that were added or modified (not deleted)
          new_files=$(git diff --name-only --diff-filter=AM $prev_commit $new_commit -- 'Clusters-Dev/AKS/Workload/**/*.yaml')
          echo "NEW_FILES=$new_files" >> $GITHUB_ENV

          # Extract the filename without its path and extension to be used as branch name, with a hardcoded prefix
          branch_name=$(basename "$new_files" | sed 's/\(.*\)\..*/\1/')
          branch_name="workload-$branch_name"
          echo "BRANCH_NAME=$branch_name" >> $GITHUB_ENV

          if [ -z "$new_files" ]; then
            echo "No YAML files found"
            exit 1
          fi

          # Get the user information (committer details)
          committer_name=$(git log -1 --pretty=format:'%an')
          committer_email=$(git log -1 --pretty=format:'%ae')
          echo "COMMITTER_NAME=$committer_name" >> $GITHUB_ENV
          echo "COMMITTER_EMAIL=$committer_email" >> $GITHUB_ENV

          # Print debug information
          echo "************************************************"
          echo "Committer Name: $committer_name"
          echo "Committer Email: $committer_email"
          echo "Branch Name: $branch_name"
          echo "Modified YAML Files: $new_files"
          echo "************************************************"






- name: Extract Parent Folder
  run: |
    # Extract the grandparent folder of the input JSON file
    parent_folder=$(basename $(dirname $(dirname $(dirname "$NEW_FILES"))))
    echo "Parent folder: $parent_folder"
    echo "BASE_FOLDER=$parent_folder" >> $GITHUB_ENV

- name: Choose Environment and Load Variables
  run: |
    # Load the correct environment file based on the parent folder
    if [ "$BASE_FOLDER" = "Clusters-Dev" ]; then
      ENV_FILE="./workflow-config/dev.env"
    elif [ "$BASE_FOLDER" = "Clusters-NonProd" ]; then
      ENV_FILE="./workflow-config/nonprod.env"
    elif [ "$BASE_FOLDER" = "Clusters-Prod" ]; then
      ENV_FILE="./workflow-config/prod.env"
    else
      echo "Invalid parent folder: $BASE_FOLDER"
      exit 1
    fi

    # Export the variables from the chosen environment file
    echo "Loading environment variables from $ENV_FILE"
    export $(grep -v '^#' "$ENV_FILE" | xargs)


- name: Extract Parent Folder
  run: |
    # Extract the parent folder of the input JSON file
    parent_folder=$(basename $(dirname $(dirname "$NEW_FILES")))
    echo "Parent folder: $parent_folder"
    echo "BASE_FOLDER=$parent_folder" >> $GITHUB_ENV

- name: Choose Environment and Load Variables
  run: |
    # Load the correct environment file based on the parent folder
    if [ "$BASE_FOLDER" = "Clusters-Dev" ]; then
      ENV_FILE="./workflow-config/dev.env"
    elif [ "$BASE_FOLDER" = "Clusters-NonProd" ]; then
      ENV_FILE="./workflow-config/nonprod.env"
    elif [ "$BASE_FOLDER" = "Clusters-Prod" ]; then
      ENV_FILE="./workflow-config/prod.env"
    else
      echo "Invalid parent folder: $BASE_FOLDER"
      exit 1
    fi

    # Export the variables from the chosen environment file
    echo "Loading environment variables from $ENV_FILE"
    export $(grep -v '^#' "$ENV_FILE" | xargs)


import json
import os
import sys
import logging
import re

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)


def load_file(filepath):
    """Load and parse a JSON file."""
    try:
        with open(filepath, 'r', encoding='utf-8') as file_handle:
            return json.load(file_handle)
    except FileNotFoundError:
        logging.error("File %s not found.", filepath)
        raise
    except json.JSONDecodeError:
        logging.error("Error decoding JSON in file %s", filepath)
        raise


def get_spec_file(input_data):
    """Get the spec file path based on the input data spec field."""
    spec_type = input_data.get("spec", "shared")  # Default to "shared" if not provided
    spec_file = f"../../../Cluster-artifacts/Spec/{spec_type}-latest.json"

    if not os.path.exists(spec_file):
        raise FileNotFoundError(f"Spec file '{spec_file}' not found for spec type: {spec_type}")
    return spec_file


def validate_input(input_data, field_mapping):
    """Validate input fields based on the rules provided in field mapping."""
    common_fields = field_mapping.get('common', {})

    for field, rules in common_fields.items():
        value = input_data.get(field)

        if not value:
            if rules.get('required', False):
                raise ValueError(
                    f"Field '{field}' is required but missing in input.json."
                )
            continue

        if not isinstance(value, str):
            raise ValueError(
                f"Field '{field}' should be a string. Got {type(value)} instead."
            )

        min_length = rules.get("min_length")
        if min_length and len(value) < min_length:
            raise ValueError(
                f"Field '{field}' is shorter than min. length of {min_length}."
            )

        max_length = rules.get("max_length")
        if max_length and len(value) > max_length:
            raise ValueError(
                f"Field '{field}' exceeds the maximum length of {max_length}."
            )

        pattern = rules.get("regex")
        if pattern and not re.match(pattern, value):
            raise ValueError(
                f"Field '{field}' does not match required pattern '{pattern}'."
            )

    logging.info("Validation successful!")


def replace_placeholders(template_data, input_data):
    """Replace placeholders in the template with values from the input data."""
    for key, value in input_data.items():
        placeholder = f"${{{key}}}"
        template_data = template_data.replace(placeholder, value)

    return template_data


def validate_paths(*paths):
    """Validate that all provided paths exist."""
    for path in paths:
        if not os.path.exists(path):
            raise FileNotFoundError(f"Path '{path}' not found.")


def create_required_folders(base_dir, folders):
    """Create required folders inside a base directory."""
    for folder in folders:
        destination_folder = os.path.join(base_dir, folder)
        os.makedirs(destination_folder, exist_ok=True)
        sample_file_path = os.path.join(destination_folder, "README.md")
        with open(sample_file_path, 'w', encoding='utf-8') as sample_file:
            sample_file.write(f"This is a sample file for {folder}.")


def generate_output(validated_data, output_dir):
    """Dynamically generate output filename and folder structure."""
    cluster_name = validated_data.get("CLUSTER_NAME")
    if not cluster_name:
        raise ValueError("CLUSTER_NAME is missing from the input data.")

    cluster_output_dir = os.path.join(output_dir, cluster_name)
    os.makedirs(cluster_output_dir, exist_ok=True)

    create_required_folders(cluster_output_dir, ["namespaces", "config", "rbac", "network"])

    output_file_name = f"{cluster_name}.yaml"
    output_file_path = os.path.join(cluster_output_dir, output_file_name)
    logging.info("Output file path: %s", output_file_path)

    return output_file_path


def write_output(output_file_path, output_data):
    """Write the processed template data to an output YAML file."""
    try:
        with open(output_file_path, 'w', encoding='utf-8') as file_handle:
            file_handle.write(output_data)
        logging.info("Generated manifest %s", output_file_path)
    except IOError as e:
        logging.error("Error writing to %s manifest file with %s", output_file_path, e)
        raise


def process_file(input_file, output_dir):
    """Process a single input file dynamically based on the spec."""
    try:
        input_data = load_file(input_file)

        # Dynamically get the correct spec file
        spec_file = get_spec_file(input_data)
        spec_data = load_file(spec_file)

        template_file = spec_data['cluster']['clusterDefinition']['template']
        mapping_file = "../../../Cluster-artifacts/Templates/FieldMapping.json"

        validate_paths(mapping_file, template_file)

        field_mapping = load_file(mapping_file)
        with open(template_file, 'r', encoding='utf-8') as template_handle:
            template_data = template_handle.read()

        validate_input(input_data, field_mapping)
        output_data = replace_placeholders(template_data, input_data)
        output_file_path = generate_output(input_data, output_dir)
        write_output(output_file_path, output_data)

    except FileNotFoundError as e:
        logging.error("File not found: %s. Error: %s", input_file, e)
    except json.JSONDecodeError as e:
        logging.error("JSON decode error in file %s. Error: %s", input_file, e)
    except ValueError as e:
        logging.error("Value error in file %s. Error: %s", input_file, e)
    except IOError as e:
        logging.error("I/O error in file %s. Error: %s", input_file, e)
    except Exception as e:
        logging.error("Unexpected error in file %s. Error: %s", input_file, e)
        raise


def main():
    """Main function to process multiple input files passed as arguments."""
    if len(sys.argv) < 3:
        print("Usage: python3 change.py input.json output_directory")
        sys.exit(1)

    input_files = sys.argv[1:-1]
    output_dir = sys.argv[-1]

    for input_file in input_files:
        if not os.path.exists(input_file):
            logging.error("Input file '%s' not found.", input_file)
            continue

        try:
            process_file(input_file, output_dir)
        except FileNotFoundError as e:
            logging.error("File not found: %s. Error: %s", input_file, e)
        except json.JSONDecodeError as e:
            logging.error("JSON decode error %s. Error: %s", input_file, e)
        except ValueError as e:
            logging.error("Value error in file %s. Error: %s", input_file, e)
        except IOError as e:
            logging.error("I/O error in file %s. Error: %s", input_file, e)
        except Exception as e:
            logging.error("Unexpected error file %s. Error: %s", input_file, e)
            raise


if __name__ == "__main__":
    main()

git fetch origin
git checkout $BRANCH_NAME

# Commit local changes if any
if ! git diff-index --quiet HEAD --; then
  git add .
  git commit -m "Automated commit"
fi

# Pull with rebase
git pull origin $BRANCH_NAME --rebase

# Push changes
git push origin $BRANCH_NAME



- name: Configure git
  run: |
    git config user.name "test-user"
    git config user.email "test-user@test.com"

- name: Stash untracked files (if any)
  run: git stash --include-untracked

- name: Switch to a new branch
  run: git checkout -b $BRANCH_NAME

- name: Pull changes and rebase
  run: git pull origin $BRANCH_NAME --rebase

- name: Reapply stashed changes (if applicable)
  run: git stash pop || true

- name: Commit and push changes
  env:
    ACCESS_TOKEN: ${{ secrets.ACCESS_TOKEN }}
  run: |
    git add .
    git commit -m "Automated commit"
    git push https://x-access-token:${{ secrets.ACCESS_TOKEN }}@github.com/${{ github.repository }} $BRANCH_NAME


- name: Commit and Push Changes
  env:
    ACCESS_TOKEN: ${{ env.ACCESS_TOKEN }}
  run: |
    git config user.name "usertest"
    git config user.email "usertest@test.com"
    
    # Stash any unstaged changes to prevent merge conflicts
    git stash
    
    # Checkout the branch and pull the latest changes from remote
    git checkout -b $BRANCH_NAME
    git pull origin $BRANCH_NAME --rebase
    
    # Apply stashed changes, if any
    git stash pop || true
    
    # Stage and commit the changes
    git add .
    git commit -m "Automated commit"
    
    # Push changes, use --force-with-lease to ensure safe push
    git push https://x-access-token:${ACCESS_TOKEN}@github.com/${{ github.repository }} $BRANCH_NAME --force-with-lease



PR_RESPONSE=$(curl -X POST \
    -H "Authorization: Bearer $ACCESS_TOKEN" \
    -H "Accept: application/vnd.github+json" \
    https://api.github.com/repos/ValueMomentum-Inc/GEICOPOC-Spec/pulls \
    -d '{
      "title": "Automated PR",
      "head": "'"${BRANCH_NAME}"'",
      "base": "main",
      "body": "Automated PR created via GitHub Actions"
    }')



- name: Create a Pull Request
  env:
    ACCESS_TOKEN: ${{ env.ACCESS_TOKEN }}
  run: |
    PR_RESPONSE=$(curl -X POST \
      -H "Authorization: Bearer ${ACCESS_TOKEN}" \
      -H "Accept: application/vnd.github+json" \
      https://api.github.com/repos/${{ github.repository }}/pulls \
      -d '{
        "title": "Automated PR",
        "head": "'"${{ github.actor }}/$BRANCH_NAME"'",
        "base": "main",
        "body": "Automated PR created via GitHub Actions"
      }')
    echo "Pull Request Response: $PR_RESPONSE"


- name: Debug PR Creation
  run: |
    echo "Branch: $BRANCH_NAME"
    echo "Repo: ${{ github.repository }}"
    echo "PR Response: $PR_RESPONSE"


"""
A module that handles input file validation, placeholder replacement,
and output generation.This is the Python package which generates,
the YAML manifest with the directory structure.
Author: Prasad Jivane
Date created/Modified: 11/09/2024
Usage of this module: python main.py input file paths
"""

import json
import re
import os
import sys
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)


def load_file(filepath):
    """Load and parse a JSON file."""
    try:
        with open(filepath, 'r', encoding='utf-8') as file_handle:
            return json.load(file_handle)
    except FileNotFoundError:
        logging.error("File %s not found.", filepath)
        raise
    except json.JSONDecodeError:
        logging.error("Error decoding JSON in file %s", filepath)
        raise


def validate_input(input_data, field_mapping):
    """
    Validate input fields based on the rules provided in field mapping
    (regex, min_length, max_length).
    """
    common_fields = field_mapping.get('common', {})

    for field, rules in common_fields.items():
        value = input_data.get(field)

        if not value:
            if rules.get('required', False):
                raise ValueError(
                    f"Field '{field}' is required but missing in input.json."
                )
            continue

        if not isinstance(value, str):
            raise ValueError(
                f"Field '{field}' should string. Got {type(value)} instead."
            )

        min_length = rules.get("min_length")
        if min_length and len(value) < min_length:
            raise ValueError(
                f"Field '{field}' is shorter than min. length of {min_length}."
            )

        max_length = rules.get("max_length")
        if max_length and len(value) > max_length:
            raise ValueError(
                f"Field '{field}' exceeds the maximum length of {max_length}."
            )

        pattern = rules.get("regex")
        if pattern and not re.match(pattern, value):
            raise ValueError(
                f"Field '{field}' not match required pattern '{pattern}'."
            )

    logging.info("Validation successful!")


def replace_placeholders(template_data, input_data):
    """
    Replace placeholders in the template with values from the input data.
    """
    for key, value in input_data.items():
        placeholder = f"${{{key}}}"
        template_data = template_data.replace(placeholder, value)

    return template_data


def validate_file_presence(mapping_file, template_file):
    """
    Validate that the mapping and template files exist.
    """
    if not mapping_file or not os.path.exists(mapping_file):
        raise FileNotFoundError(f"Mapping file '{mapping_file}' not found.")
    if not template_file or not os.path.exists(template_file):
        raise FileNotFoundError(f"Template file '{template_file}' not found.")


def generate_output(validated_data, output_dir):
    """
    Dynamically generate output filename based on input JSON data.
    """
    cluster_name = validated_data.get("CLUSTER_NAME")
    if not cluster_name:
        raise ValueError("CLUSTER_NAME is missing from the input data.")

    cluster_output_dir = os.path.join(output_dir, cluster_name)
    os.makedirs(cluster_output_dir, exist_ok=True)

    required_folders = ["namespaces", "config", "rbac", "network"]
    for folder in required_folders:
        destination_folder = os.path.join(cluster_output_dir, folder)
        os.makedirs(destination_folder, exist_ok=True)
        sample_file_path = os.path.join(destination_folder, "README.md")
        with open(sample_file_path, 'w', encoding='utf-8') as sample_file:
            sample_file.write(f"This is a sample file for {folder}.")

    output_file_name = f"{cluster_name}.yaml"
    output_file_path = os.path.join(cluster_output_dir, output_file_name)
    logging.info("Output file path: %s", output_file_path)

    return output_file_path


def write_output(output_file_path, output_data):
    """
    Write the processed template data to an output YAML file.
    Raises:
        IOError: If there is a problem writing to the file.
    """
    try:
        with open(output_file_path, 'w', encoding='utf-8') as file_handle:
            file_handle.write(output_data)
        logging.info("Generated manifest %s", output_file_path)
    except IOError as e:
        logging.error(
            "Error writing to %s manifest file with %s",
            output_file_path,
            e
        )
        raise


def process_file(input_file, output_dir):
    """
    Process a single input file: validation, placeholder replacement,
    and output generation.
    """
    try:
        input_data = load_file(input_file)

        mapping_file = input_data.get("mapping")
        template_file = input_data.get("template")

        validate_file_presence(mapping_file, template_file)

        field_mapping = load_file(mapping_file)
        with open(template_file, 'r', encoding='utf-8') as template_handle:
            template_data = template_handle.read()

        validate_input(input_data, field_mapping)
        output_data = replace_placeholders(template_data, input_data)
        output_file_path = generate_output(input_data, output_dir)
        write_output(output_file_path, output_data)

    except FileNotFoundError as e:
        logging.error("File not found: %s. Error: %s", input_file, e)
    except json.JSONDecodeError as e:
        logging.error("JSON decode error in file %s. Error: %s", input_file, e)
    except ValueError as e:
        logging.error("Value error in file %s. Error: %s", input_file, e)
    except IOError as e:
        logging.error("I/O error in file %s. Error: %s", input_file, e)
    except Exception as e:
        logging.error("Unexpected error file %s. Error: %s", input_file, e)
        raise


def main():
    """
    Main function to process multiple input files passed as arguments.
    """
    if len(sys.argv) < 3:
        print("Usage: python3 main.py input.json, output_directory")
        sys.exit(1)

    # input_files = sys.argv[1:]
    # output_dir = sys.argv[2:]
    input_files = sys.argv[1:-1]
    output_dir = sys.argv[-1]

    for input_file in input_files:
        if not os.path.exists(input_file):
            logging.error("Input file '%s' not found.", input_file)
            continue

        try:
            process_file(input_file, output_dir)
        except FileNotFoundError as e:
            logging.error("File not found: %s. Error: %s", input_file, e)
        except json.JSONDecodeError as e:
            logging.error("JSON decode error %s. Error: %s", input_file, e)
        except ValueError as e:
            logging.error("Value error in file %s. Error: %s", input_file, e)
        except IOError as e:
            logging.error("I/O error in file %s. Error: %s", input_file, e)
        except Exception as e:
            logging.error("Unexpected error file %s. Error: %s", input_file, e)
            raise


if __name__ == "__main__":
    main()

